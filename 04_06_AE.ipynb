{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_06_AE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamagami/anomaly-detection/blob/main/04_06_AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LloPJKYhRrFa"
      },
      "source": [
        "# オートエンコーダによる時系列異常検知"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwX5od76SD7j"
      },
      "source": [
        "#  Google Driveを使う場合の例\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DIR=\"drive/My Drive/Colab Notebooks/異常検知サンプルコード/\"\n",
        "!wget https://dl.dropbox.com/s/x3fmb9mxr4xkip3/qtdbsel102.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwAwFbwfRrFa"
      },
      "source": [
        "### 必要なモジュールのimport"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SCqTKMJRrFa"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "#from keras.layers import Dense #, Dropout, Flatten\n",
        "#from keras.layers.convolutional import Conv1D, UpSampling1D\n",
        "#from keras.layers.pooling import MaxPooling1D\n",
        "#from keras.utils import np_utils\n",
        "#from keras.utils import plot_model\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y5NtdgERrFb"
      },
      "source": [
        "### セグメントの切り出し関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZLRIYK6RrFb"
      },
      "source": [
        "def segdata(lst, dim):\n",
        "    emb = np.empty((0,dim), float)\n",
        "    for i in range(lst.size - dim + 1):\n",
        "        tmp = np.array(lst[i:i+dim])[::-1].reshape((1,-1)) #セグメントの切り出し，時系列反転，appendのための2次ベクトル化\n",
        "        emb = np.append( emb, tmp, axis=0)\n",
        "    return emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wEN7O9sRrFb"
      },
      "source": [
        "### データ読み込み，パラメータ設定\n",
        " Keoghらの心電図のデータ  http://www.cs.ucr.edu/~eamonn/discords/qtdbsel102.txt\n",
        " Keogh, E., Lin, J. and Fu, A.: HOT SAX : Efficiently Finding the Most Unusual Time Series Subsequence, in Proceedings of the Fifth IEEE International Conference on Data Mining, ICDM 05, pp.226-233."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj1sxa6ZRrFb"
      },
      "source": [
        "LEN=3000  #分析区間\n",
        "WLEN=100 #セグメントのサイズ\n",
        "SP=0         #学習用データの開始点\n",
        "AP=3000   #テスト用データの開始点　個のデータの場合 4250ポイント付近に異常がある"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au6fJ50NRrFb"
      },
      "source": [
        "data = np.loadtxt(\"qtdbsel102.txt\",delimiter=\"\\t\")\n",
        "print(\"データ数:\",data.shape[0],\"  次元数:\",data.shape[1])\n",
        "\n",
        "#元データは3次元の時系列，3次のデータ(indexとしては2)を指定して学習/テストデータに分割\n",
        "train_org = data[SP:SP+LEN, 2]      #学習用データとして 1～2999サンプル区間を使用\n",
        "test_org  = data[AP:AP+LEN, 2]  #テスト用データとして3000～5999サンプルを使用"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bIaQW9_RrFb"
      },
      "source": [
        "### 窓関数の設定と切り出し"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyflNfFIRrFb"
      },
      "source": [
        "seglen = WLEN   #Window size\n",
        "#winlenの単位で1ポイントずつずらした2次元表現に変換\n",
        "x_train = segdata(train_org, seglen)\n",
        "x_test  = segdata(test_org,seglen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SDrkSP2RrFb"
      },
      "source": [
        "### AEのデザイン"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtyISnSFRrFb"
      },
      "source": [
        "input_img = Input(shape=(100,))\n",
        "encoded = Dense(64, activation='linear')(input_img)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "encoded = Dense(16, activation='relu',name='middle_code')(encoded)\n",
        "\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(64, activation='relu')(decoded)\n",
        "decoded = Dense(100, activation='linear',name='output_code')(decoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijDU56TrRrFb"
      },
      "source": [
        "### AEの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGyXksKRrFc"
      },
      "source": [
        "history=autoencoder.fit(np.array(x_train), np.array(x_train),\n",
        "                epochs=1000,\n",
        "                batch_size=50,\n",
        "                shuffle=True,\n",
        "                verbose=1,\n",
        "                validation_split=0.1)\n",
        "                #validation_data=(np.array(x_valid), np.array(x_valid)))\n",
        "\n",
        "model_json_str = autoencoder.to_json()\n",
        "open('ae_model.json', 'w').write(model_json_str)\n",
        "autoencoder.save_weights('ae_weights.h5');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1QXzHzORrFc"
      },
      "source": [
        "### 学習曲線の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_lPHbJIRrFc"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Brt0C2MRrFc"
      },
      "source": [
        "### 波形と復元誤差の描画"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw27vNadRrFc"
      },
      "source": [
        "#encoded_sig = Model(input_img, encoded\n",
        "decoded_trainsig = autoencoder.predict(x_train)\n",
        "decoded_testsig = autoencoder.predict(x_test)\n",
        "trainloss=[]\n",
        "testloss=[]\n",
        "for i,d in enumerate(decoded_trainsig):\n",
        "    #plt.plot(d)\n",
        "    #plt.show()\n",
        "    d_trainsig = d-x_train[i]\n",
        "    d_testsig  = decoded_testsig[i]-x_test[i]\n",
        "    #print(np.linalg.norm(d_sig,ord=100))\n",
        "    trainloss.append(np.linalg.norm(d_trainsig, ord=WLEN))\n",
        "    testloss.append(np.linalg.norm(d_testsig, ord=WLEN))\n",
        "#print(loss)\n",
        "plt.plot(train_org)\n",
        "plt.plot(trainloss)\n",
        "plt.show()\n",
        "plt.plot(test_org)\n",
        "plt.plot(testloss)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjtf2m8sRrFc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC3w8LTkRrFc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWbjcsG-RrFc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}